---
title: ACI BGP Design
layout: default
nav_order: 3
---

# ACI BGP Design

The ACI BGP design is identical regardless of the Cilium design we choose.
## L3Out physical connectivity

There is no strict requirement of the physical connectivity for the L3Out as long as it provides the required redundancy level.

This design implements L3 connectivity with the use of the floating SVI feature. This feature allows us to configure an L3Out without specifying logical interfaces, thus removing the need to configure multiple L3Out logical interfaces to maintain routing when VMs move from one host to another. Floating SVI is supported for VMware vSphere Distributed Switch (VDS) as of Cisco ACI Release 4.2(1) and on physical domains as of Cisco ACI Release 5.0(1). It is recommended to use the physical domains approach for the following reasons:

* It can support any hypervisor.
* It can support mixed mode clusters (VMs and bare-metal).

This is not a strict requirement, and, if all nodes are running in a VMware environment, there are no technical reasons not to use VMM integration.
Using floating SVI also relaxes the requirement of not having any Layer-2 communications between the `nodes`; this allows the design to use:

* A single subnet for all the `nodes`
* A single encapsulation (VLAN) for all the `nodes`


### BGP Dynamic Neighbors

All the `nodes` configured for BGP peering will share the same AS number. By sharing the same AS number, we can use the BGP Dynamic Neighbors capability, which allows for the dynamic and automated establishment of peering sessions between routers. Instead of requiring manual configuration of individual neighbor statements for each peer, this feature utilizes a `listener` (in our case, the ACI fabric) to accept incoming BGP connections from a predefined IP subnet.

### BGP Graceful Restart

Both ACI and Cilium will be configured to use BGP Graceful Restart. When a BGP speaker restarts its BGP process or when the BGP process crashes, neighbors will not discard the received paths from the speaker, ensuring that connectivity is not impacted as long as the data plane is still correctly programmed.

### BFD With Cilium Enterprise 

Bidirectional Forwarding Detection (BFD) is a network protocol used to detect faults in the path between two forwarding engines, such as routers, and is commonly used in conjunction with routing protocols like BGP (Border Gateway Protocol). BFD enhances BGP operations by providing quick and reliable detection of link failures, enabling faster failover and improved network resilience. Thanks to this capability we can achieve sub second failure detection for our BGP Peering.

### BGP timers tuning with Cilium OSS

Currently, Cilium OSS does not support Bidirectional Forwarding Detection (BFD), so, to achieve faster convergence, we must rely on BGP timers tuning. Since the number of `ingress nodes` is limited, this should not pose any scalability issues on the border leaves.

The ACI BGP timers should be set to 1s/3s to ensure faster convergence.

**Note:** Do not perform this step if you choose to use Cilium Enterprise with BFD



### Next Hop Propagate and Ignore IGP Metric - ACI 6.1(2)+ Required

{: .warning } 
This two configs are recommended for the *Simplicity Design* and a **requirement** for the *Advanced Design*.

Even if an external router is connected under a non-anchor leaf node, traffic from a Cisco ACI internal endpoint sent to the external destination goes to an anchor leaf node and then is redirected to the external router through the non-anchor leaf node, which represents a suboptimal traffic path. Furthermore as explained in the [Pre ACI 6.1(2) Limitation](#pre-aci-612-limitation) section, routes generated by nodes directly connected to the anchor nodes are preferred over routes from nodes not directly connected to the anchor nodes, and this could lead to nodes not being utilized.

Both of the above issues can be removed by enabling these two features:

* Next-hop propagation: this configuration is applied only to the anchor leaf nodes and enables them to redistribute the external prefixes inside the Cisco ACI fabric with the next-hop IP address of the external router announcing these external prefixes. That way, the compute leaf nodes receives and install in their forwarding tables the external prefixes with the external router's IP address (In our case the Cilium Ingress Gateway) as the next-hop.

* Ignore IGP Metric: When you configure this option the BGP best path algorithm does not consider the IGP metric of the Next Hop resulting in both Anchor and Non-Anchor connected nodes to be selected as next hops ensure all the K8s nodes are used.

Note the Next Hop Propagate  feature was introduced in ACI release 5.0(1) however until ACI 6.1(2) there was no support for Ignore IGP Metric and for this design to work both features are required. Configuring only Next Hop Propagate will not be sufficient and if you are running on ACI 6.1.1 or lower the recommended design is to simply connect the `ingress nodes` only to `ACI Anchor nodes`. 

### Max BGP ECMP path (optional)

By default, ACI installs up to 16 eBGP/iBGP ECMP paths. If more than 16 `nodes` are required, ACI can be configured to install up to 64 ECMP paths.

### BGP hardening (optional)

To protect the ACI system against potential Kubernetes BGP misconfigurations, the following settings are recommended:

* Enable BGP password authentication
* Set the maximum AS limit to one:
  * Per the eBGP design, the AS path should always be one.
* Configure BGP import route control to accept only the expected external-services subnets.
* (Optional) Set a limit on the number of received prefixes from the nodes.


[Next](/docs/examples/advanced/){: .btn }
{: .text-right }