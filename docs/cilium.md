---
title: Cilium Designs
layout: default
nav_order: 2
---
# Cilium Overview

[Cilium](https://docs.isovalent.com/project/cilium/index.html) is an open-source project to provide networking, security, and observability for cloud-native environments such as Kubernetes clusters and other container orchestration platforms.

At the foundation of Cilium is a Linux kernel technology called eBPF, which enables the dynamic insertion of powerful security, visibility, and networking control logic into the Linux kernel. eBPF is used to provide high-performance networking, multicluster, and multicloud capabilities, advanced load balancing, transparent encryption, extensive network security capabilities, transparent observability, and much more.

In this section we are gonna learn about the features that are required from a Cilium perspective for our two designs.
## Table of contents
{: .no_toc .text-delta }
1. TOC
{:toc}
---

## Common Features

### POD to POD Communications:
**Native Routing, Auto Direct Node Routes and Endpoint Routes**

By enabling these 3 features we can implement an efficient packet forwarding between PODs without requiring additional encapsulation or overlay networks. By having direct routes to each pod's IP address subnet, PODs can communicate directly over the existing Layer 2 infrastructure, reducing latency and potential overhead associated with tunneling protocols.

In this context, a Floating SVI (Switched Virtual Interface) will act as a shared L2 Domain for the cluster nodes. The Auto Direct Node Routes feature thus leverages the existing L2 topology to streamline pod-to-pod communication, ensuring that packet delivery is both efficient and straightforward. It eliminates the need for exposing the POD Subnet to the broader network fabric, thereby maintaining a cleaner and simple network architecture.

Endpoint Routes enables direct routing to the `veth` pairs without requiring to route via the `cilium_host` interface further increasing the performance.

This configuration aligns with Cilium's ethos of providing high-performance, scalable, and simple networking for Kubernetes environments. By integrating closely with the Linux kernel's routing capabilities, Cilium can offer robust networking solutions without necessitating complex configurations or additional network infrastructure.

### Cilium BGP Control Plane

[Cilium BGP Control Plane](https://docs.isovalent.com/configuration-guide/networking/bgpv2/index.html) provides a way for Cilium to advertise routes to connected routers by using the Border Gateway Protocol (BGP). Cilium BGP Control Plane makes pod networks and/or load-balancer services of type LoadBalancer reachable from outside the cluster for environments that support BGP. In Cilium, the BGP Control Plane does not program the Linux host data path, so it cannot be used to establish IP reachability within the cluster or to external IPs.


#### Pre ACI 6.1(2) Limitation
{: .no_toc }
In ACI software releases prior to 6.1(2), it is required to either:
* peer the `ingress nodes` only with directly attached anchor nodes
* peer the `ingress nodes` only with NON directly attached anchor nodes

This is required becasue routes generated by nodes directly connected to the anchor nodes are preferred over routes from nodes not directly connected to the anchor nodes, and this could lead to nodes not being utilized.

For example: if we have exposed, from our ingress nodes, a service with IP `1.1.1.1` over BGP, anchor `leaf101` will only install one ECMP path for `1.1.1.1/32` through `192.168.2.1`, because the locally attached route is preferred.
![pre6.1(2)-limitation](../images/pre612-limitation.png)
Pre ACI 6.1(2) limitation
 
This limitation is removed in ACI 6.1(2) and is strongly recommended to use ACI 6.1(2) or newer.

### Cilium Egress Gateway

The [egress gateway features](https://docs.isovalent.com/configuration-guide/networking/egress-gateway/introduction.html) allows for redirecting traffic originating from pods and destined to specific CIDRs outside the cluster to be routed through particular nodes (from now on called "gateway nodes").

When the egress gateway feature is enabled and egress gateway policies are in place, packets leaving the cluster are masqueraded with selected, predictable IPs (`egressIP`) associated with the gateway nodes. As an example, this feature can be used with legacy firewalls to allow traffic to legacy infrastructure only from specific pods within a given namespace. These pods typically have ever-changing IP addresses. 

#### Egress Gateway and ACI
{: .no_toc }
We can harness the capabilities of ACI Endpoint Security Groups (ESGs) to develop an efficient network design with the following structure:

* Dedicated ESG for Egress Gateway Traffic: The nodes performing egress will be configured with an additional Subnet that can be then classified into ESGs 
* Cilium Egress Gateway Policies: Implement Cilium Egress Gateway policies to associate specific namespaces with designated gateway nodes, each with a fixed egress IP address. This mapping ensures consistent and predictable IP addresses for the Outbound cluster traffic.
* ESG Classification on Egress IPs: Apply ESG classification to the egress IPs to streamline network management and policy enforcement, enhancing the security and control over outbound traffic at a namespace level. 

It is important to note that this design specifically addresses traffic leaving the cluster. Internal cluster traffic will remain unaffected by these configurations. This ensures that while outbound traffic is tightly controlled and secured, cluster-local communications continue to operate without interruption.

![Egress Gateway and ESGs](../images/egress.png)
Egress Gateway traffic flows

*Note:* The Egress Gateway feature can be configured to advertise the egress IP via BGP, which is a valid design choice. However, since ACI is limited to a maximum of 250 external EPGs per leaf per L3Out, it is more scalable to classify the egress IP within an ESG.

### XDP Acceleration

[The XDP Acceleration](https://docs.cilium.io/en/stable/network/kubernetes/kubeproxy-free/#loadbalancer-nodeport-xdp-acceleration) supports NodePort, LoadBalancer services and services with externalIPs for the case where the arriving request needs to be forwarded and the backend is located on a remote node. This feature was introduced in Cilium version 1.8 at the XDP (eXpress Data Path) layer where eBPF is operating directly in the networking driver instead of a higher layer.
The majority of drivers supporting 10G or higher rates also support native XDP on a recent kernel and this feature should be enabled.

## Advanced Design Only Features

### MagLev

Incorporating advanced load balancing mechanisms into Kubernetes clusters is essential for maintaining optimal performance and efficient resource utilization. The integration of Cilium with Maglev hashing provides a robust solution for consistent and efficient load distribution, particularly in scenarios involving Equal-Cost Multi-Path (ECMP) routing.

Maglev consistent hashing minimizes such disruptions by ensuring that each load balancing node has a consistent view and ordering for the backend lookup table such that selecting the backend through the packet's 5-tuple hash will always result in forwarding the traffic to the very same backend without having to synchronize state with the other nodes. This not only improves resiliency in case of failures but also achieves better load balancing properties since newly added nodes will make the same consistent backend selection throughout the cluster.

![cilium-maglev](../images/cilium-maglev.gif)

Aside from that, the Maglev consistent hashing algorithm ensures even balancing among backends as well as minimal disruptions in the case where backends are added or removed. Specifically, a flow is highly likely to choose the same backend after adding or removing a backend for a service as it did before the operation. Upon backend removal, the backend lookup tables are reprogrammed with minimal changes for unrelated backends, that is, typical configurations provide the upper bound of at most 1% tolerable difference in the reassignments.

This is particularly important for any network fabric that, like ACI, don't support ECMP Resilient Hashing.

To successfully implement Maglev hashing within a Kubernetes environment using Cilium, a key requirement is that Kubernetes nodes can select the Pod IP as the destination IP for traffic routing. This capability is crucial for maintaining consistent and efficient load balancing across the network. Additionally, for optimal performance and reduced latency, the node where the Pod resides should be able to reply directly back to the network. This direct response mechanism requires the implementation of Direct Server Return (DSR).

See https://cilium.io/blog/2020/11/10/cilium-19/ for a detailed blog post.

### Direct Server Return

When Direct Server Return (DSR) mode is enabled in Cilium, the efficiency of handling node-external traffic is significantly improved. In this mode, once a request reaches a backend pod located on a remote node, the response is sent directly back to the client from the pod, bypassing the original node that received the request. This eliminates the additional hop needed for reverse SNAT, reducing latency and improving overall network performance.

The DSR mode provides two key benefits:

* Preservation of Source IP: Since the backend pod sends the response directly to the client, the original source IP of the client is preserved. This is particularly beneficial for security and monitoring purposes, as it allows network policies and logging mechanisms on the backend node to accurately identify and match the client's IP address. This can be crucial for implementing fine-grained security policies and for troubleshooting.
* Reduced Latency and Load: By removing the need for the response to pass back through the node that initially received the request, the latency is reduced. This also decreases the processing load on that node, as it does not have to handle reverse SNAT for responses, allowing it to manage incoming requests more efficiently.

Overall, enabling DSR in Cilium optimizes network traffic flow, enhances security, and ensures more efficient resource usage within the Kubernetes cluster. It's a strategic choice for environments where performance and accurate client identification are priorities.

[Next](/docs/simplicity_design/){: .btn }
{: .text-right }
